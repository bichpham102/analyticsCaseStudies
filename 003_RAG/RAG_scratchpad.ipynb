{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Source: 'Building LLMs for Production' by Louis-Francois Bouchard, Louie Peters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Install related packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a LangChain Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader \n",
    "\n",
    "text =\"\"\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It's similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file \n",
    "with open('my_file.txt','w') as file: \n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from local file \n",
    "\n",
    "loader = TextLoader('my_file.txt')      # Create an instance of TextLoader class \n",
    "docs_from_file = loader.load()          # Call the load method to load the text from the file\n",
    "\n",
    "print(len(docs_from_file))              # Print the number of documents loaded from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the documents into Chunks with Text Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chunk_overlap` is the number of characters that overlap between two chunks. \n",
    "\n",
    ">It preserves context and improves coherence by ensuring that important information is not cut off at the boundaries of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 369, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "\n",
    "# Create a text splitter instance \n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "# Split the text into chunks\n",
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Setup a vector store & Create an embedding for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A vector store is a system to store embeddings, allowing us to query them.\n",
    "\n",
    "> Chunking is done because LLMs typically have a limited context window, and storing smaller chunks improves retrieval accuracy and ensures relevant sections are retrieved.\n",
    "\n",
    "We'll utilize the Deep Lake vector store, offered by Activeloop. They provide a cloud-based vector store solution, but other options like Chroma DB would also be suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the \"OPENAI_API_KEY\" environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://bichpham102/langchain_course_indexers_retrievers already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:42<00:00, 42.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bichpham102/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (8, 1536)  float32   None   \n",
      "    id        text      (8, 1)      str     None   \n",
      " metadata     json      (8, 1)      str     None   \n",
      "   text       text      (8, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['000c4df6-78c3-11ef-89ac-6045bd1d6a3b',\n",
       " '000c4f36-78c3-11ef-89ac-6045bd1d6a3b']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake \n",
    "\n",
    "# Signup and Get your DeepLake API key  (https://app.activeloop.ai/), then save API Key / Secret before executing the following code \n",
    "\n",
    "# create a DeepLake dataset \n",
    "\n",
    "my_activeloop_org_id = \"bichpham102\" # TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_dataset_name = 'langchain_course_indexers_retrievers'\n",
    "\n",
    "dataset_path = f'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)\n",
    "# the documents (docs) and their corresponding embeddings will be generated and stored in the new DeepLake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create & work with a LangChain retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once created the retriever, we can use the `RetrievalQA` class to define a question answering chain using external data source and start with question-answering. \n",
    "\n",
    "###### 1. **`chain_type='stuff'`**\n",
    "   - **How it works**: This method takes **all retrieved documents** and \"stuffs\" them into a single prompt to the language model (LLM). It concatenates the documents and the query into one large input and then sends that to the LLM to generate a response.\n",
    "   - **Use case**: Suitable when the **total size of documents is small** enough to fit within the LLM’s context window (the maximum input size the model can process).\n",
    "   - **Advantages**: It's **simple and fast** because it only makes one call to the LLM.\n",
    "   - **Limitations**: This approach **fails** when the combined document size exceeds the context length of the model, leading to truncation or incomplete processing of the documents.\n",
    "   - **Best for**: Short documents or queries that need to be processed all at once【42†source】【45†source】.\n",
    "\n",
    "###### 2. **`chain_type='map-reduce'`**\n",
    "   - **How it works**: Once the relevant chunks are retrieved from the vector store, the map-reduce process treats each retrieved chunk **independently**. It runs the LLM on each chunk to generate partial answers (the map step), and then **aggregates** (reduces) these partial results to form a final answer (the reduce step).\n",
    "   - **Use case**: Ideal for scenarios where the documents are **too large** to fit into the context window at once.\n",
    "   - **Advantages**: It allows for **scaling** to larger datasets because each chunk can be processed separately. It reduces the risk of important information being missed due to context limitations.\n",
    "   - **Limitations**: This approach may lead to **inconsistencies** across chunks or loss of context between parts of the document.\n",
    "   - **Best for**: Handling larger documents that exceed the model’s context window【43†source】【44†source】.\n",
    "\n",
    "###### 3. **`chain_type='refine'`**\n",
    "   - **How it works**: In this method, the LLM processes the first document, generates an answer, and then **iteratively refines** that answer by incorporating information from each subsequent document. Each document is used to improve or adjust the previous answer.\n",
    "   - **Use case**: Best for situations where you want the model to **build upon** or **refine** an answer by looking at documents one-by-one.\n",
    "   - **Advantages**: This method is beneficial when **context continuity** is essential because the model refines its response with each new document. It ensures that **new insights** from each document are incorporated into the final answer.\n",
    "   - **Limitations**: It can be **slow** as the model processes each document individually and performs multiple iterations.\n",
    "   - **Best for**: Complex queries where iterative improvements in the answer based on additional information are crucial【42†source】【45†source】.\n",
    "\n",
    "###### Summary of Use Cases:\n",
    "- **`stuff`**: Use when documents are small enough to fit into the context window. Simple and fast but limited by input size.\n",
    "- **`map-reduce`**: Ideal for large documents that need to be broken into chunks. Efficient for handling large datasets but may lead to slight inconsistencies.\n",
    "- **`refine`**: Best for complex answers that require refinement over time by integrating information from multiple documents. Provides thoroughness but is slower due to multiple iterations.\n",
    "\n",
    "These methods allow you to tailor the retrieval and answering process based on the size and complexity of the documents you're working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4a. RetrievalQA with base retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever() \n",
    "# calling the method as_retriever() on the vector store instance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo') \n",
    "\n",
    "# create a retrieval chain \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm\n",
    "    ,chain_type='stuff'\n",
    "    ,retriever=retriever \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google plans to challenge OpenAI by opening up its advanced AI language model, PaLM, to developers. By launching an API for PaLM and providing a range of AI enterprise tools, Google aims to help businesses generate various forms of content from simple natural language prompts. This move positions Google to compete with OpenAI and its renowned GPT-3 model in the field of AI language processing.\n"
     ]
    }
   ],
   "source": [
    "query = 'How Google plans to challenge OpenAI?'\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4b. RetrievalQA with base retriever and additional LLMChainExtractor as compressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever \n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# create a GPT-3 wrapper instance \n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo') \n",
    "\n",
    "# create compressor for the retriever \n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# Wrap the original retriever with ContextualCompressionRetriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor\n",
    "    ,base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google opens up its AI language model PaLM to challenge OpenAI and GPT-3. Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\n"
     ]
    }
   ],
   "source": [
    "#  retrieves and compresses relevant documents -> a list of document objects\n",
    "query = 'How Google plans to challenge OpenAI?'\n",
    "# You are working directly with the retrieved and compressed documents, \n",
    "# no Q&A or further language model processing happens at this stage.\n",
    "retrieved_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Output: It returns a list of document objects. Each document in this list represents a relevant text retrieved from the underlying data store (like a vector store), compressed for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/langchain/chains/llm.py:275: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google plans to challenge OpenAI by opening up its AI language model PaLM. This move allows researchers and developers to access and utilize Google's powerful language model, potentially competing with OpenAI's models in the field of artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm \n",
    "    ,chain_type='stuff'\n",
    "    ,retriever=compression_retriever \n",
    ")\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Data Ingestion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5a. TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader \n",
    "\n",
    "loader = TextLoader('my_file.txt', encoding='utf-8')    # Create an instance of TextLoader class\n",
    "documents = loader.load()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An example of what the documents list would contain after running the TextLoader `[Document(page_content='<FILE_CONTENT>', metadata={'source': 'file_path.txt'})]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5b. PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages:  3\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "\n",
    "loader = PyPDFLoader('Storage Options GCP.pdf')\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print('number of pages: ' ,len(pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5c. SeleniumURLLoader (URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the `load()` method is used with the `SeleniumURLLoader`, it returns a collection of Document instances, each containing the content fetched from the web pages. These Document instances have a `page_content` attribute, which includes the text extracted from the HTML, and a metadata attribute that stores the source URL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SeleniumURLLoader class in LangChain has the following attributes :\n",
    "\n",
    "• `URLs (List[str])`: A list of URLs that the loader will access.\n",
    "\n",
    "• `continue_on_failure (bool, default=True)`: Determines whether the loader should continue processing other URLs in case of a failure.\n",
    "\n",
    "• `browser (str, default=“chrome”)`: Choice of browser for loading the URLs. Options typically include ‘Chrome’ or ‘Firefox’.\n",
    "\n",
    "• `executable_path (Optional[str], default=None)`: The path to the browser’s executable file.\n",
    "\n",
    "• `headless (bool, default=True)`: Specifies whether the browser should operate in headless mode, meaning it runs without a visible user interface.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q unstructured selenium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='' metadata={'source': 'https://www.youtube.com/watch?v=TFa539R09EQ&t=139s'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import SeleniumURLLoader \n",
    "\n",
    "urls = [\n",
    "    \"https://www.youtube.com/watch?v=TFa539R09EQ&t=139s\",\n",
    "    \"https://www.youtube.com/watch?v=6Zv6A_9urh4&t=112s\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls = urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5d. Google Drive Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `GoogleDriveLoader`, you need to set up the necessary credentials and tokens:\n",
    "\n",
    "• The loader typically looks for the credentials.json file in the `~/.credentials/credentials.json` directory. You can specify a different path using the `credentials_file` keyword argument.\n",
    "\n",
    "• For the token, the token.json file is created automatically on the loader’s first use and follows a similar path convention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`recursive=False `: This means the loader will only access files directly within the specified folder_id and not look into any of its subfolders. It does not recurse into deeper folder structures within the specified folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GoogleDriveLoader \n",
    "\n",
    "# instantiate the GoogleDriveLoader \n",
    "loader = GoogleDriveLoader(\n",
    "    folder_id = 'folder_id'\n",
    "    ,recursive=False # will not look into subfolders \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text Splitter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6a. `CharacterTextSplitter` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Ingest documents \n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader \n",
    "loader = PyPDFLoader('The One Page Linux Manual.pdf') \n",
    "pages = loader.load_and_split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking with `CharacterTextSplitter``\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\\nVersion 3.0 May 1999 squadron@powerup.com.au\\nStarting & Stopping\\nshutdown -h now Shutdown the system now and do not\\nreboot\\nhalt Stop all processes - same as above\\nshutdown -r 5 Shutdown the system in 5 minutes and\\nreboot\\nshutdown -r now Shutdown the system now and reboot\\nreboot Stop all processes and then reboot - same\\nas above\\nstartx Start the X system\\nAccessing & mounting file systems\\nmount -t iso9660 /dev/cdrom\\n/mnt/cdromMount the device cdrom\\nand call it cdrom under the\\n/mnt directory\\nmount -t msdos /dev/hdd\\n/mnt/ddriveMount hard disk “d” as a\\nmsdos file system and call\\nit ddrive under the /mnt\\ndirectory\\nmount -t vfat /dev/hda1\\n/mnt/cdriveMount hard disk “a” as a\\nVFAT file system and call it\\ncdrive under the /mnt\\ndirectory\\numount /mnt/cdrom Unmount the cdrom\\nFinding files and text within files\\nfind / -name  fname Starting with the root directory, look\\nfor the file called fname\\nfind / -name ”*fname* ” Starting with the root directory, look\\nfor the file containing the string fname\\nlocate missingfilename Find a file called missingfilename\\nusing the locate command - this\\nassumes you have already used the\\ncommand updatedb (see next)\\nupdatedb Create or update the database of files\\non all file systems attached to the linux\\nroot directory\\nwhich missingfilename Show the subdirectory containing the\\nexecutable file  called missingfilename\\ngrep textstringtofind\\n/dirStarting with the directory called dir ,\\nlook for and list all files containing\\ntextstringtofind\\nThe X Window System\\nxvidtune Run the X graphics tuning utility\\nXF86Setup Run the X configuration menu with\\nautomatic probing of graphics cards\\nXconfigurator Run another X configuration menu with\\nautomatic probing of graphics cards\\nxf86config Run a text based X configuration menu\\nMoving, copying, deleting & viewing files\\nls -l List files in current directory using\\nlong format\\nls -F List files in current directory and\\nindicate the file type\\nls -laC List all files in current directory in\\nlong format and display in columnsrm name Remove a file or directory called\\nname\\nrm -rf name Kill off an entire directory and all it’s\\nincludes files and subdirectories\\ncp filename\\n/home/dirnameCopy the file called filename to the\\n/home/dirname directory\\nmv filename\\n/home/dirnameMove the file called filename to the\\n/home/dirname directory\\ncat filetoview Display the file called filetoview\\nman -k keyword Display man pages containing\\nkeyword\\nmore filetoview Display the file called filetoview one\\npage at a time, proceed to next page\\nusing the spacebar\\nhead filetoview Display the first 10 lines of the file\\ncalled filetoview\\nhead -20 filetoview Display the first 20 lines of the file\\ncalled filetoview\\ntail filetoview Display the last 10 lines of the file\\ncalled filetoview\\ntail -20 filetoview Display the last 20 lines of the file\\ncalled filetoview\\nInstalling software for Linux\\nrpm -ihv name.rpm Install the rpm package called name\\nrpm -Uhv name.rpm Upgrade the rpm package called\\nname\\nrpm -e package Delete the rpm package called\\npackage\\nrpm -l package List the files in the package called\\npackage\\nrpm -ql package List the files and state the installed\\nversion of the package called\\npackage\\nrpm -i --force package Reinstall the rpm package called\\nname having deleted parts of it (not\\ndeleting using rpm -e)\\ntar -zxvf archive.tar.gz or\\ntar -zxvf archive.tgzDecompress the files contained in\\nthe zipped and tarred archive called\\narchive\\n./configure Execute the script preparing the\\ninstalled files for compiling\\nUser Administration\\nadduser accountname Create a new user call accountname\\npasswd accountname Give accountname a new password\\nsu Log in as superuser from current login\\nexit Stop being superuser and revert to\\nnormal user\\nLittle known tips and tricks\\nifconfig List ip addresses for all devices on\\nthe machine\\napropos subject List manual pages for subject\\nusermount Executes graphical application for\\nmounting and unmounting file\\nsystems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "You have 2 documents\n",
      "Preview:\n",
      "THE ONE     PAGE LINUX MANUALA summary of useful Linux commands\n",
      "Version 3.0 May 1999 squadron@powerup.com.au\n",
      "Starting & Stopping\n",
      "shutdown -h now Shutdown the system now and do not\n",
      "reboot\n",
      "halt Stop all processes - same as above\n",
      "shutdown -r 5 Shutdown the system in 5 minutes and\n",
      "reboot\n",
      "shutdown -r now Shutdown the system now and reboot\n",
      "reboot Stop all processes and then reboot - same\n",
      "as above\n",
      "startx Start the X system\n",
      "Accessing & mounting file systems\n",
      "mount -t iso9660 /dev/cdrom\n",
      "/mnt/cdromMount the device cdrom\n",
      "and call it cdrom under the\n",
      "/mnt directory\n",
      "mount -t msdos /dev/hdd\n",
      "/mnt/ddriveMount hard disk “d” as a\n",
      "msdos file system and call\n",
      "it ddrive under the /mnt\n",
      "directory\n",
      "mount -t vfat /dev/hda1\n",
      "/mnt/cdriveMount hard disk “a” as a\n",
      "VFAT file system and call it\n",
      "cdrive under the /mnt\n",
      "directory\n",
      "umount /mnt/cdrom Unmount the cdrom\n",
      "Finding files and text within files\n",
      "find / -name  fname Starting with the root directory, look\n",
      "for the file called fname\n",
      "find / -name ”*fname* ” Starting with the root directory, look\n",
      "for the file containing the string fname\n",
      "locate missingfilename Find a file called missingfilename\n",
      "using the locate command - this\n",
      "assumes you have already used the\n",
      "command updatedb (see next)\n",
      "updatedb Create or update the database of files\n",
      "on all file systems attached to the linux\n",
      "root directory\n",
      "which missingfilename Show the subdirectory containing the\n",
      "executable file  called missingfilename\n",
      "grep textstringtofind\n",
      "/dirStarting with the directory called dir ,\n",
      "look for and list all files containing\n",
      "textstringtofind\n",
      "The X Window System\n",
      "xvidtune Run the X graphics tuning utility\n",
      "XF86Setup Run the X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "Xconfigurator Run another X configuration menu with\n",
      "automatic probing of graphics cards\n",
      "xf86config Run a text based X configuration menu\n",
      "Moving, copying, deleting & viewing files\n",
      "ls -l List files in current directory using\n",
      "long format\n",
      "ls -F List files in current directory and\n",
      "indicate the file type\n",
      "ls -laC List all files in current directory in\n",
      "long format and display in columnsrm name Remove a file or directory called\n",
      "name\n",
      "rm -rf name Kill off an entire directory and all it’s\n",
      "includes files and subdirectories\n",
      "cp filename\n",
      "/home/dirnameCopy the file called filename to the\n",
      "/home/dirname directory\n",
      "mv filename\n",
      "/home/dirnameMove the file called filename to the\n",
      "/home/dirname directory\n",
      "cat filetoview Display the file called filetoview\n",
      "man -k keyword Display man pages containing\n",
      "keyword\n",
      "more filetoview Display the file called filetoview one\n",
      "page at a time, proceed to next page\n",
      "using the spacebar\n",
      "head filetoview Display the first 10 lines of the file\n",
      "called filetoview\n",
      "head -20 filetoview Display the first 20 lines of the file\n",
      "called filetoview\n",
      "tail filetoview Display the last 10 lines of the file\n",
      "called filetoview\n",
      "tail -20 filetoview Display the last 20 lines of the file\n",
      "called filetoview\n",
      "Installing software for Linux\n",
      "rpm -ihv name.rpm Install the rpm package called name\n",
      "rpm -Uhv name.rpm Upgrade the rpm package called\n",
      "name\n",
      "rpm -e package Delete the rpm package called\n",
      "package\n",
      "rpm -l package List the files in the package called\n",
      "package\n",
      "rpm -ql package List the files and state the installed\n",
      "version of the package called\n",
      "package\n",
      "rpm -i --force package Reinstall the rpm package called\n",
      "name having deleted parts of it (not\n",
      "deleting using rpm -e)\n",
      "tar -zxvf archive.tar.gz or\n",
      "tar -zxvf archive.tgzDecompress the files contained in\n",
      "the zipped and tarred archive called\n",
      "archive\n",
      "./configure Execute the script preparing the\n",
      "installed files for compiling\n",
      "User Administration\n",
      "adduser accountname Create a new user call accountname\n",
      "passwd accountname Give accountname a new password\n",
      "su Log in as superuser from current login\n",
      "exit Stop being superuser and revert to\n",
      "normal user\n",
      "Little known tips and tricks\n",
      "ifconfig List ip addresses for all devices on\n",
      "the machine\n",
      "apropos subject List manual pages for subject\n",
      "usermount Executes graphical application for\n",
      "mounting and unmounting file\n",
      "systems\n"
     ]
    }
   ],
   "source": [
    "print(texts[0]) \n",
    "\n",
    "print(f'You have {len(texts)} documents')\n",
    "print('Preview:')\n",
    "print(texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6b. `RecursiveCharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "\n",
    "loader = PyPDFLoader('The One Page Linux Manual.pdf') \n",
    "pages = loader.load_and_split() \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50 \n",
    "    ,chunk_overlap=10 \n",
    "    ,length_function=len # calculates the length of chunks\n",
    "    # The default is len, which counts the number of characters\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='THE ONE     PAGE LINUX MANUALA summary of useful' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of useful Linux commands' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Version 3.0 May 1999 squadron@powerup.com.au' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Starting & Stopping' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -h now Shutdown the system now and do' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and do not' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot\\nhalt Stop all processes - same as above' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -r 5 Shutdown the system in 5 minutes' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='5 minutes and' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='shutdown -r now Shutdown the system now and' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='now and reboot' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='reboot Stop all processes and then reboot - same' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='as above\\nstartx Start the X system' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Accessing & mounting file systems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mount -t iso9660 /dev/cdrom' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/cdromMount the device cdrom' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and call it cdrom under the\\n/mnt directory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mount -t msdos /dev/hdd' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/ddriveMount hard disk “d” as a' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='msdos file system and call' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='it ddrive under the /mnt\\ndirectory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='directory\\nmount -t vfat /dev/hda1' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/mnt/cdriveMount hard disk “a” as a' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='VFAT file system and call it' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='cdrive under the /mnt\\ndirectory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='directory\\numount /mnt/cdrom Unmount the cdrom' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Finding files and text within files' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='find / -name  fname Starting with the root' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the root directory, look' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='for the file called fname' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='find / -name ”*fname* ” Starting with the root' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the root directory, look' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='for the file containing the string fname' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='locate missingfilename Find a file called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called missingfilename' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='using the locate command - this' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='assumes you have already used the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='command updatedb (see next)' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='updatedb Create or update the database of files' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='on all file systems attached to the linux' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='root directory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='which missingfilename Show the subdirectory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='containing the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='executable file  called missingfilename' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='grep textstringtofind' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/dirStarting with the directory called dir ,' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='look for and list all files containing' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='textstringtofind\\nThe X Window System' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='xvidtune Run the X graphics tuning utility' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='XF86Setup Run the X configuration menu with' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='automatic probing of graphics cards' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Xconfigurator Run another X configuration menu' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='menu with' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='automatic probing of graphics cards' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='xf86config Run a text based X configuration menu' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Moving, copying, deleting & viewing files' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -l List files in current directory using' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='long format' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -F List files in current directory and' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='indicate the file type' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ls -laC List all files in current directory in' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='long format and display in columnsrm name Remove' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='Remove a file or directory called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rm -rf name Kill off an entire directory and all' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='and all it’s' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='includes files and subdirectories\\ncp filename' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirnameCopy the file called filename to the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirname directory\\nmv filename' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirnameMove the file called filename to the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/home/dirname directory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='cat filetoview Display the file called filetoview' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='man -k keyword Display man pages containing' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='keyword' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='more filetoview Display the file called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview one' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='page at a time, proceed to next page' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='using the spacebar' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='head filetoview Display the first 10 lines of the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of the file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='head -20 filetoview Display the first 20 lines of' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='lines of the file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tail filetoview Display the last 10 lines of the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='of the file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tail -20 filetoview Display the last 20 lines of' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='lines of the file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called filetoview\\nInstalling software for Linux' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -ihv name.rpm Install the rpm package called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='called name' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -Uhv name.rpm Upgrade the rpm package called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name\\nrpm -e package Delete the rpm package called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -l package List the files in the package' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -ql package List the files and state the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='state the installed' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='version of the package called\\npackage' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='rpm -i --force package Reinstall the rpm package' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='package called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='name having deleted parts of it (not' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='deleting using rpm -e)' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tar -zxvf archive.tar.gz or' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='tar -zxvf archive.tgzDecompress the files' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the files contained in' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the zipped and tarred archive called\\narchive' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='./configure Execute the script preparing the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='installed files for compiling\\nUser Administration' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='adduser accountname Create a new user call' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='user call accountname' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='passwd accountname Give accountname a new' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='a new password' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='su Log in as superuser from current login' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='exit Stop being superuser and revert to' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='normal user\\nLittle known tips and tricks' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='ifconfig List ip addresses for all devices on' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='the machine' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='apropos subject List manual pages for subject' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='usermount Executes graphical application for' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='mounting and unmounting file\\nsystems' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 0}\n",
      "page_content='/sbin/e2fsck hda5 Execute the filesystem check' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='check utility' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='on partition hda5' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='fdformat /dev/fd0H1440 Format the floppy disk in' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='disk in device fd0' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='tar -cMf /dev/fd0 Backup the contents of the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='of the current' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='directory and subdirectories to' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='multiple floppy disks' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='tail -f /var/log/messages Display the last 10' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='last 10 lines of the system' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='log.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cat /var/log/dmesg Display the file containing' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the boot' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='time messages - useful for locating' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='problems. Alternatively, use the\\ndmesg  command.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='* wildcard - represents everything. eg.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cp from/* to  will copy all files in the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='from directory to the to directory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='? Single character wildcard. eg.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='cp config.? /configs will copy all files' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='beginning with the name config. in' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the current directory to the directory' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='named configs.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='[xyz] Choice of character wildcards. eg.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='ls [xyz]* will list all files in the current' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='directory starting with the letter x, y,\\nor z.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='linux single At the lilo prompt, start in single' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='in single user' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='mode. This is useful if you have' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='forgotten your password. Boot in' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='single user mode, then run the\\npasswd  command.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='ps List current processes' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='kill 123 Kill a specific process eg. kill 123' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Configuration files and what they do' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/profile System wide environment variables' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='variables for' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='all users.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/fstab List of devices and their associated' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='mount' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='points. Edit this file to add cdroms, DOS' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='partitions and floppy drives at startup.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/motd Message of the day broadcast to all' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='to all users' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='at login.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='etc/rc.d/rc.local Bash script that is executed at' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='at the end of' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='login process. Similar to autoexec.bat in\\nDOS.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/HOSTNAME Conatins full hostname including' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='including domain.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/cron.* There are 4 directories that' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='that automatically' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='execute all scripts within the directory at' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='intervals of hour, day, week or month.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/hosts A list of all know host names and IP' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='addresses on the machine.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/httpd/conf Paramters for the Apache web' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='web server' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/inittab Specifies the run level that the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='that the machine' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='should boot into.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/resolv.conf Defines IP addresses of DNS' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='of DNS servers.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/smb.conf Config file for the SAMBA server.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='server. Allows' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='file and print sharing with Microsoft\\nclients.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='clients.\\n/etc/X11/XF86Confi' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='gConfig file for X -Windows.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='~/.xinitrc Defines the windows manager loaded by' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='X. ~ refers to user’s home directory.File' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='permissions' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='If the command ls -l is given, a long list of' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='list of file names is' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='displayed. The first column in this list details' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='details the permissions' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='applying to the file. If a permission is missing' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='missing for a owner,' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='group of other, it is represented by - eg.  drwxr' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='drwxr -x—x' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Read = 4\\nWrite = 2' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Execute = 1File permissions are altered by giving' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='by giving the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod command and the appropriate' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='octal code for each user type. eg' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod 7 6 4 filename will make the file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called filename R+W+X for the owner,' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='R+W for the group and R for others.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod 7 5 5 Full permission for the owner, read' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='read and' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='execute access for the group and others.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='chmod +x filename Make the file called filename' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='filename executable' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='to all users.\\nX Shortcuts - (mainly for Redhat)' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Control|Alt  + or - Increase or decrease the' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='the screen' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='resolution. eg. from 640x480 to\\n800x600' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Alt | escape Display list of active windows' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control F8 Resize the selected window' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Right click on desktop\\nbackgroundDisplay menu' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control Altr Refresh the screen' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Shift|Control Altx Start an xterm session' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Printing' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd start Start the print daemon' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd stop Stop the print daemon' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='/etc/rc.d/init.d/lpd' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='statusDisplay status of the print daemon' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lpq Display jobs in print queue' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lprm Remove jobs from queue\\nlpr Print a file' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='lpc Printer control tool' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='man subject | lpr Print the manual page called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called subject' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='as plain text' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='man -t subject | lpr Print the manual page called' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='called subject' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='as Postscript output' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='printtool Start X printer setup' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='setup interface~/.Xdefaults Define configuration' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='for some X -' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='applications. ~ refers to user’s home\\ndirectory.' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='Get your own Official Linux Pocket Protector -' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='- includes' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='handy command summary. Visit:' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n",
      "page_content='www.powerup.com.au/~squadron' metadata={'source': 'The One Page Linux Manual.pdf', 'page': 1}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `RecursiveCharacterTextSplitter` can also split by tokens if used with a Token Counter.\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "# install `transformers` \n",
    "\n",
    "!pip install transformers\n",
    "\n",
    "# modify the `length_function` to use a token-based counter\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer (e.g., BERT tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Function to count tokens\n",
    "def token_counter(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Load and split the PDF\n",
    "loader = PyPDFLoader('The One Page Linux Manual.pdf')\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# Use the token counter for the length function\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,  # This is now 50 tokens\n",
    "    chunk_overlap=10,  # 10 tokens of overlap\n",
    "    length_function=token_counter  # Counts tokens instead of characters\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6c. `NLTK TextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='This is documentation for LangChain v0.1, which is no longer actively maintained.\\n\\nFor the current stable version, see this version (Latest).\\n\\nWeb scraping\\n\\nUse case\\u200b\\n\\nWeb research is one of the killer LLM applications:\\n\\nUsers have highlighted it as one of his top desired AI tools.\\n\\nOSS repos like gpt-researcher are growing in popularity.\\n\\nOverview\\u200b\\n\\nGathering content from the web has a few components:\\n\\nSearch: Query to url (e.g., using GoogleSearchAPIWrapper).\\n\\nLoading: Url to HTML (e.g., using AsyncHtmlLoader, AsyncChromiumLoader, etc).\\n\\nTransforming: HTML to formatted text (e.g., using HTML2Text or Beautiful Soup).\\n\\nQuickstart\\u200b\\n\\npip install -q langchain-openai langchain playwright beautifulsoup4\\nplaywright install\\n\\n# Set env var OPENAI_API_KEY or load from a .env file:\\n# import dotenv\\n# dotenv.load_dotenv()\\n\\nScraping HTML content using a headless instance of Chromium.\\n\\nThe async nature of the scraping process is handled using Python\\'s asyncio library.\\n\\nThe actual interaction with the web pages is handled by Playwright.\\n\\nfrom langchain_community.document_loaders import AsyncChromiumLoader\\nfrom langchain_community.document_transformers import BeautifulSoupTransformer\\n\\n# Load HTML\\nloader = AsyncChromiumLoader([\"https://www.wsj.com\"])\\nhtml = loader.load()\\n\\nAPI Reference:\\n\\nAsyncChromiumLoader\\n\\nBeautifulSoupTransformer\\n\\nScrape text content tags such as <p>, <li>, <div>, and <a> tags from the HTML content:\\n\\n<p>: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.\\n\\n<li>: The list item tag. It is used within ordered (<ol>) and unordered (<ul>) lists to define individual items within the list.\\n\\n<div>: The division tag. It is a block-level element used to group other inline or block-level elements.\\n\\n<a>: The anchor tag. It is used to define hyperlinks.\\n\\n<span>: an inline container used to mark up a part of a text, or a part of a document.\\n\\nFor many news websites (e.g., WSJ, CNN), headlines and summaries are all in <span> tags.\\n\\n# Transform\\nbs_transformer = BeautifulSoupTransformer()\\ndocs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])\\n\\n# Result\\ndocs_transformed[0].page_content[0:500]\\n\\n\\'English EditionEnglish中文 (Chinese)日本語 (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are \\'\\n\\nThese Documents now are staged for downstream usage in various LLM apps, as discussed below.\\n\\nLoader\\u200b\\n\\nAsyncHtmlLoader\\u200b\\n\\nThe AsyncHtmlLoader uses the aiohttp library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping.\\n\\nAsyncChromiumLoader\\u200b\\n\\nThe AsyncChromiumLoader uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions.\\n\\nChromium is one of the browsers supported by Playwright, a library used to control browser automation.\\n\\nHeadless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping.\\n\\nfrom langchain_community.document_loaders import AsyncHtmlLoader\\n\\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\\nloader = AsyncHtmlLoader(urls)\\ndocs = loader.load()\\n\\nAPI Reference:\\n\\nAsyncHtmlLoader\\n\\nTransformer\\u200b\\n\\nHTML2Text\\u200b\\n\\nHTML2Text provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation.\\n\\nIt\\'s best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements.\\n\\nBeautiful Soup\\u200b\\n\\nBeautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.\\n\\nIt\\'s suited for cases where you want to extract specific information and clean up the HTML content according to your needs.\\n\\nfrom langchain_community.document_loaders import AsyncHtmlLoader\\n\\nurls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"]\\nloader = AsyncHtmlLoader(urls)\\ndocs = loader.load()\\n\\nAPI Reference:\\n\\nAsyncHtmlLoader\\n\\nFetching pages: 100%|#############################################################################################################| 2/2 [00:00<00:00,  7.01it/s]\\n\\nfrom langchain_community.document_transformers import Html2TextTransformer\\n\\nhtml2text = Html2TextTransformer()\\ndocs_transformed = html2text.transform_documents(docs)\\ndocs_transformed[0].page_content[0:500]\\n\\nAPI Reference:\\n\\nHtml2TextTransformer\\n\\n\"Skip to main content  Skip to navigation\\\\n\\\\n<\\\\n\\\\n>\\\\n\\\\nMenu\\\\n\\\\n## ESPN\\\\n\\\\n  * Search\\\\n\\\\n  *   * scores\\\\n\\\\n  * NFL\\\\n  * MLB\\\\n  * NBA\\\\n  * NHL\\\\n  * Soccer\\\\n  * NCAAF\\\\n  * …\\\\n\\\\n    * Women\\'s World Cup\\\\n    * LLWS\\\\n    * NCAAM\\\\n    * NCAAW\\\\n    * Sports Betting\\\\n    * Boxing\\\\n    * CFL\\\\n    * NCAA\\\\n    * Cricket\\\\n    * F1\\\\n    * Golf\\\\n    * Horse\\\\n    * MMA\\\\n    * NASCAR\\\\n    * NBA G League\\\\n    * Olympic Sports\\\\n    * PLL\\\\n    * Racing\\\\n    * RN BB\\\\n    * RN FB\\\\n    * Rugby\\\\n    * Tennis\\\\n    * WNBA\\\\n    * WWE\\\\n    * X Games\\\\n    * XFL\\\\n\\\\n  * More\"\\n\\nScraping with extraction\\u200b\\n\\nLLM with function calling\\u200b\\n\\nWeb scraping is challenging for many reasons.\\n\\nOne of them is the changing nature of modern websites\\' layouts and content, which requires modifying scraping scripts to accommodate the changes.\\n\\nUsing Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change.\\n\\nWe\\'re using gpt-3.5-turbo-0613 to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing).\\n\\nWe\\'re also keeping temperature at 0 to keep randomness of the LLM down.\\n\\nfrom langchain_openai import ChatOpenAI\\n\\nllm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\\n\\nAPI Reference:\\n\\nChatOpenAI\\n\\nDefine a schema\\u200b\\n\\nNext, you define a schema to specify what kind of data you want to extract.\\n\\nHere, the key names matter as they tell the LLM what kind of information they want.\\n\\nSo, be as detailed as possible.\\n\\nIn this example, we want to scrape only news article\\'s name and summary from The Wall Street Journal website.\\n\\nfrom langchain.chains import create_extraction_chain\\n\\nschema = {\\n    \"properties\": {\\n        \"news_article_title\": {\"type\": \"string\"},\\n        \"news_article_summary\": {\"type\": \"string\"},\\n    },\\n    \"required\": [\"news_article_title\", \"news_article_summary\"],\\n}\\n\\n\\ndef extract(content: str, schema: dict):\\n    return create_extraction_chain(schema=schema, llm=llm).run(content)\\n\\nAPI Reference:\\n\\ncreate_extraction_chain\\n\\nRun the web scraper w/ BeautifulSoup\\u200b\\n\\nAs shown above, we\\'ll be using BeautifulSoupTransformer.\\n\\nimport pprint\\n\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\n\\ndef scrape_with_playwright(urls, schema):\\n    loader = AsyncChromiumLoader(urls)\\n    docs = loader.load()\\n    bs_transformer = BeautifulSoupTransformer()\\n    docs_transformed = bs_transformer.transform_documents(\\n        docs, tags_to_extract=[\"span\"]\\n    )\\n    print(\"Extracting content with LLM\")\\n\\n    # Grab the first 1000 tokens of the site\\n    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\\n        chunk_size=1000, chunk_overlap=0\\n    )\\n    splits = splitter.split_documents(docs_transformed)\\n\\n    # Process the first split\\n    extracted_content = extract(schema=schema, content=splits[0].page_content)\\n    pprint.pprint(extracted_content)\\n    return extracted_content\\n\\n\\nurls = [\"https://www.wsj.com\"]\\nextracted_content = scrape_with_playwright(urls, schema=schema)\\n\\nAPI Reference:\\n\\nRecursiveCharacterTextSplitter\\n\\nExtracting content with LLM\\n[{\\'news_article_summary\\': \\'The Americans will remain under house arrest until \\'\\n                          \\'they are allowed to return to the U.S. in coming \\'\\n                          \\'weeks, following a monthslong diplomatic push by \\'\\n                          \\'the Biden administration.\\',\\n  \\'news_article_title\\': \\'Four Americans Released From Iranian Prison\\'},\\n {\\'news_article_summary\\': \\'Price pressures continued cooling last month, with \\'\\n                          \\'the CPI rising a mild 0.2% from June, likely \\'\\n                          \\'deterring the Federal Reserve from raising interest \\'\\n                          \\'rates at its September meeting.\\',\\n  \\'news_article_title\\': \\'Cooler July Inflation Opens Door to Fed Pause on \\'\\n                        \\'Rates\\'},\\n {\\'news_article_summary\\': \\'The company has decided to eliminate 27 of its 30 \\'\\n                          \\'clothing labels, such as Lark & Ro and Goodthreads, \\'\\n                          \\'as it works to fend off antitrust scrutiny and cut \\'\\n                          \\'costs.\\',\\n  \\'news_article_title\\': \\'Amazon Cuts Dozens of House Brands\\'},\\n {\\'news_article_summary\\': \\'President Biden’s order comes on top of a slowing \\'\\n                          \\'Chinese economy, Covid lockdowns and rising \\'\\n                          \\'tensions between the two powers.\\',\\n  \\'news_article_title\\': \\'U.S. Investment Ban on China Poised to Deepen Divide\\'},\\n {\\'news_article_summary\\': \\'The proposed trial date in the \\'\\n                          \\'election-interference case comes on the same day as \\'\\n                          \\'the former president’s not guilty plea on \\'\\n                          \\'additional Mar-a-Lago charges.\\',\\n  \\'news_article_title\\': \\'Trump Should Be Tried in January, Prosecutors Tell \\'\\n                        \\'Judge\\'},\\n {\\'news_article_summary\\': \\'The CEO who started in June says the platform has \\'\\n                          \\'“an entirely different road map” for the future.\\',\\n  \\'news_article_title\\': \\'Yaccarino Says X Is Watching Threads but Has Its Own \\'\\n                        \\'Vision\\'},\\n {\\'news_article_summary\\': \\'Students foot the bill for flagship state \\'\\n                          \\'universities that pour money into new buildings and \\'\\n                          \\'programs with little pushback.\\',\\n  \\'news_article_title\\': \\'Colleges Spend Like There’s No Tomorrow. ‘These \\'\\n                        \\'Places Are Just Devouring Money.’\\'},\\n {\\'news_article_summary\\': \\'Wildfires fanned by hurricane winds have torn \\'\\n                          \\'through parts of the Hawaiian island, devastating \\'\\n                          \\'the popular tourist town of Lahaina.\\',\\n  \\'news_article_title\\': \\'Maui Wildfires Leave at Least 36 Dead\\'},\\n {\\'news_article_summary\\': \\'After its large armored push stalled, Kyiv has \\'\\n                          \\'fallen back on the kind of tactics that brought it \\'\\n                          \\'success earlier in the war.\\',\\n  \\'news_article_title\\': \\'Ukraine Uses Small-Unit Tactics to Retake Captured \\'\\n                        \\'Territory\\'},\\n {\\'news_article_summary\\': \\'President Guillermo Lasso says the Aug. 20 election \\'\\n                          \\'will proceed, as the Andean country grapples with \\'\\n                          \\'rising drug gang violence.\\',\\n  \\'news_article_title\\': \\'Ecuador Declares State of Emergency After \\'\\n                        \\'Presidential Hopeful Killed\\'},\\n {\\'news_article_summary\\': \\'This year’s hurricane season, which typically runs \\'\\n                          \\'from June to the end of November, has been \\'\\n                          \\'difficult to predict, climate scientists said.\\',\\n  \\'news_article_title\\': \\'Atlantic Hurricane Season Prediction Increased to \\'\\n                        \\'‘Above Normal,’ NOAA Says\\'},\\n {\\'news_article_summary\\': \\'The NFL is raising the price of its NFL+ streaming \\'\\n                          \\'packages as it adds the NFL Network and RedZone.\\',\\n  \\'news_article_title\\': \\'NFL to Raise Price of NFL+ Streaming Packages as It \\'\\n                        \\'Adds NFL Network, RedZone\\'},\\n {\\'news_article_summary\\': \\'Russia is planning a moon mission as part of the \\'\\n                          \\'new space race.\\',\\n  \\'news_article_title\\': \\'Russia’s Moon Mission and the New Space Race\\'},\\n {\\'news_article_summary\\': \\'Tapestry’s $8.5 billion acquisition of Capri would \\'\\n                          \\'create a conglomerate with more than $12 billion in \\'\\n                          \\'annual sales, but it would still lack the \\'\\n                          \\'high-wattage labels and diversity that have fueled \\'\\n                          \\'LVMH’s success.\\',\\n  \\'news_article_title\\': \"Why the Coach and Kors Marriage Doesn\\'t Scare LVMH\"},\\n {\\'news_article_summary\\': \\'The Supreme Court has blocked Purdue Pharma’s $6 \\'\\n                          \\'billion Sackler opioid settlement.\\',\\n  \\'news_article_title\\': \\'Supreme Court Blocks Purdue Pharma’s $6 Billion \\'\\n                        \\'Sackler Opioid Settlement\\'},\\n {\\'news_article_summary\\': \\'The Social Security COLA is expected to rise in \\'\\n                          \\'2024, but not by a lot.\\',\\n  \\'news_article_title\\': \\'Social Security COLA Expected to Rise in 2024, but \\'\\n                        \\'Not by a Lot\\'}]\\n\\nWe can compare the headlines scraped to the page:\\n\\nLooking at the LangSmith trace, we can see what is going on under the hood:\\n\\nIt\\'s following what is explained in the extraction.\\n\\nWe call the information_extraction function on the input text.\\n\\nIt will attempt to populate the provided schema from the url content.\\n\\nResearch automation\\u200b\\n\\nRelated to scraping, we may want to answer specific questions using searched content.\\n\\nWe can automate the process of web research using a retriever, such as the WebResearchRetriever.\\n\\nCopy requirements from here:\\n\\npip install -r requirements.txt\\n\\nSet GOOGLE_CSE_ID and GOOGLE_API_KEY.\\n\\nfrom langchain.retrievers.web_research import WebResearchRetriever\\nfrom langchain_chroma import Chroma\\nfrom langchain_community.utilities import GoogleSearchAPIWrapper\\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\\n\\nAPI Reference:\\n\\nWebResearchRetriever\\n\\nGoogleSearchAPIWrapper\\n\\nChatOpenAI\\n\\nOpenAIEmbeddings\\n\\n# Vectorstore\\nvectorstore = Chroma(\\n    embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\"\\n)\\n\\n# LLM\\nllm = ChatOpenAI(temperature=0)\\n\\n# Search\\nsearch = GoogleSearchAPIWrapper()\\n\\nInitialize retriever with the above tools to:\\n\\nUse an LLM to generate multiple relevant search queries (one LLM call)\\n\\nExecute a search for each query\\n\\nChoose the top K links per query (multiple search calls in parallel)\\n\\nLoad the information from all chosen links (scrape pages in parallel)\\n\\nIndex those documents into a vectorstore\\n\\nFind the most relevant documents for each original generated search query\\n\\n# Initialize\\nweb_research_retriever = WebResearchRetriever.from_llm(\\n    vectorstore=vectorstore, llm=llm, search=search\\n)\\n\\n# Run\\nimport logging\\n\\nlogging.basicConfig()\\nlogging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO)\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\n\\nuser_input = \"How do LLM Powered Autonomous Agents work?\"\\nqa_chain = RetrievalQAWithSourcesChain.from_chain_type(\\n    llm, retriever=web_research_retriever\\n)\\nresult = qa_chain({\"question\": user_input})\\nresult\\n\\nAPI Reference:\\n\\nRetrievalQAWithSourcesChain\\n\\nINFO:langchain.retrievers.web_research:Generating questions for Google Search ...\\nINFO:langchain.retrievers.web_research:Questions for Google Search (raw): {\\'question\\': \\'How do LLM Powered Autonomous Agents work?\\', \\'text\\': LineList(lines=[\\'1. What is the functioning principle of LLM Powered Autonomous Agents?\\\\n\\', \\'2. How do LLM Powered Autonomous Agents operate?\\\\n\\'])}\\nINFO:langchain.retrievers.web_research:Questions for Google Search: [\\'1. What is the functioning principle of LLM Powered Autonomous Agents?\\\\n\\', \\'2. How do LLM Powered Autonomous Agents operate?\\\\n\\']\\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\\nINFO:langchain.retrievers.web_research:Search results: [{\\'title\\': \\'LLM Powered Autonomous Agents | Hacker News\\', \\'link\\': \\'https://news.ycombinator.com/item?id=36488871\\', \\'snippet\\': \\'Jun 26, 2023 ... Exactly. A temperature of 0 means you always pick the highest probability token (i.e. the \"max\" function), while a temperature of 1 means you\\\\xa0...\\'}]\\nINFO:langchain.retrievers.web_research:Searching for relevant urls ...\\nINFO:langchain.retrievers.web_research:Search results: [{\\'title\\': \"LLM Powered Autonomous Agents | Lil\\'Log\", \\'link\\': \\'https://lilianweng.github.io/posts/2023-06-23-agent/\\', \\'snippet\\': \\'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2) by\\\\xa0...\\'}]\\nINFO:langchain.retrievers.web_research:New URLs to load: []\\nINFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...\\n\\n{\\'question\\': \\'How do LLM Powered Autonomous Agents work?\\',\\n \\'answer\\': \"LLM-powered autonomous agents work by using LLM as the agent\\'s brain, complemented by several key components such as planning, memory, and tool use. In terms of planning, the agent breaks down large tasks into smaller subgoals and can reflect and refine its actions based on past experiences. Memory is divided into short-term memory, which is used for in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. These agents have been used in various applications, including scientific discovery and generative agents simulation.\",\\n \\'sources\\': \\'\\'}\\n\\nGoing deeper\\u200b\\n\\nHere\\'s a app that wraps this retriever with a lighweight UI.\\n\\nQuestion answering over a website\\u200b\\n\\nTo answer questions over a specific website, you can use Apify\\'s Website Content Crawler Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs, and extract text content from the web pages.\\n\\nIn the example below, we will deeply crawl the Python documentation of LangChain\\'s Chat LLM models and answer a question over it.\\n\\nFirst, install the requirements pip install apify-client langchain-openai langchain\\n\\nNext, set OPENAI_API_KEY and APIFY_API_TOKEN in your environment variables.\\n\\nThe full code follows:\\n\\nfrom langchain.indexes import VectorstoreIndexCreator\\nfrom langchain_community.docstore.document import Document\\nfrom langchain_community.utilities import ApifyWrapper\\n\\napify = ApifyWrapper()\\n# Call the Actor to obtain text from the crawled webpages\\nloader = apify.call_actor(\\n    actor_id=\"apify/website-content-crawler\",\\n    run_input={\"startUrls\": [{\"url\": \"/docs/integrations/chat/\"}]},\\n    dataset_mapping_function=lambda item: Document(\\n        page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}\\n    ),\\n)\\n\\n# Create a vector store based on the crawled data\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\n\\n# Query the vector store\\nquery = \"Are any OpenAI chat models integrated in LangChain?\"\\nresult = index.query(query)\\nprint(result)\\n\\nAPI Reference:\\n\\nVectorstoreIndexCreator\\n\\nDocument\\n\\nApifyWrapper\\n\\n Yes, LangChain offers integration with OpenAI chat models. You can use the ChatOpenAI class to interact with OpenAI models.\\n\\nHelp us out by providing feedback on this documentation page:' metadata={'source': 'https://python.langchain.com/v0.1/docs/use_cases/web_scraping/'}\n"
     ]
    }
   ],
   "source": [
    "# SeleniumURLLoader: Loads the text from the specified URL\n",
    "from langchain.document_loaders import SeleniumURLLoader \n",
    "\n",
    "urls = [\n",
    "    \"https://python.langchain.com/v0.1/docs/use_cases/web_scraping/\"\n",
    "    ,\"https://www.restack.io/docs/langchain-knowledge-langchain-web-scraper\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls = urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 789, which is longer than the specified 500\n",
      "Created a chunk of size 1300, which is longer than the specified 500\n",
      "Created a chunk of size 512, which is longer than the specified 500\n",
      "Created a chunk of size 1240, which is longer than the specified 500\n",
      "Created a chunk of size 1291, which is longer than the specified 500\n",
      "Created a chunk of size 745, which is longer than the specified 500\n",
      "Created a chunk of size 757, which is longer than the specified 500\n",
      "Created a chunk of size 517, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 has 46 chunks\n",
      "Document 2 has 70 chunks\n"
     ]
    }
   ],
   "source": [
    "# Intializae the NLTKTextSplitter \n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500)\n",
    "\n",
    "# Iterate over the documents and split them into chunks\n",
    "all_texts = []\n",
    "for i, doc in enumerate(data):\n",
    "    doc_content = doc.page_content\n",
    "    texts = text_splitter.split_text(doc_content) \n",
    "    all_texts.extend(texts) \n",
    "    print(f'Document {i+1} has {len(texts)} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    }
   ],
   "source": [
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Url to HTML (e.g., using AsyncHtmlLoader, AsyncChromiumLoader, etc).\n",
      "\n",
      "Transforming: HTML to formatted text (e.g., using HTML2Text or Beautiful Soup).\n",
      "\n",
      "Quickstart​\n",
      "\n",
      "pip install -q langchain-openai langchain playwright beautifulsoup4\n",
      "playwright install\n",
      "\n",
      "# Set env var OPENAI_API_KEY or load from a .env file:\n",
      "# import dotenv\n",
      "# dotenv.load_dotenv()\n",
      "\n",
      "Scraping HTML content using a headless instance of Chromium.\n"
     ]
    }
   ],
   "source": [
    "# print the first chunk of the first document\n",
    "print(all_texts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OSS repos like gpt-researcher are growing in popularity.\n",
      "\n",
      "Overview​\n",
      "\n",
      "Gathering content from the web has a few components:\n",
      "\n",
      "Search: Query to url (e.g., using GoogleSearchAPIWrapper).\n",
      "\n",
      "Loading: Url to HTML (e.g., using AsyncHtmlLoader, AsyncChromiumLoader, etc).\n",
      "\n",
      "Transforming: HTML to formatted text (e.g., using HTML2Text or Beautiful Soup).\n"
     ]
    }
   ],
   "source": [
    "print(all_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6d. `SpacyTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.10.18)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The SpaCy model (like en_core_web_sm) is needed because the SpacyTextSplitter relies on SpaCy's natural language processing (NLP) capabilities to analyze and understand the structure of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.10.18)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (73.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# download the language model \n",
    "!python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Introduction\\n\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain's open-source building blocks, components, and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\n\\nProductionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\\n\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.\\n\\nConcretely, the framework consists of the following open-source libraries:\\n\\nlangchain-core: Base abstractions and LangChain Expression Language.\\n\\nlangchain-community: Third party integrations.\\n\\nPartner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\\n\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\n\\nLangGraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.\\n\\nLangServe: Deploy LangChain chains as REST APIs.\\n\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.\\n\\nnote\\n\\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\n\\nTutorials\\u200b\\n\\nIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials section. This is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\n\\nBuild a Chatbot\\n\\nBuild an Agent\\n\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\n\\nHow-to guides\\u200b\\n\\nHere you’ll find short answers to “How do I….?” types of questions. These how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference. However, these guides will help you quickly accomplish common tasks.\\n\\nCheck out LangGraph-specific how-tos here.\\n\\nConceptual guide\\u200b\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! Here you'll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out this page.\\n\\nAPI reference\\u200b\\n\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\nEcosystem\\u200b\\n\\n🦜🛠️ LangSmith\\u200b\\n\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n🦜🕸️ LangGraph\\u200b\\n\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\nAdditional resources\\u200b\\n\\nVersions\\u200b\\n\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\nSecurity\\u200b\\n\\nRead up on security best practices to make sure you're developing safely with LangChain.\\n\\nIntegrations\\u200b\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\n\\nContributing\\u200b\\n\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n\\nEdit this page\\n\\nWas this page helpful?\\n\\nYou can also leave detailed feedback on GitHub.\" metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "# SeleniumURLLoader: Loads the text from the specified URL\n",
    "from langchain.document_loaders import SeleniumURLLoader \n",
    "\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/introduction/\"\n",
    "    ,\"https://python.langchain.com/docs/versions/v0_3/\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls = urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 has 9 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1186, which is longer than the specified 500\n",
      "Created a chunk of size 509, which is longer than the specified 500\n",
      "Created a chunk of size 645, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 2 has 20 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import SpacyTextSplitter \n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500, chunk_overlap=25) \n",
    "\n",
    "all_texts = []  \n",
    "for i, doc in enumerate(data): \n",
    "    doc_content = doc.page_content \n",
    "    texts = text_splitter.split_text(doc_content) \n",
    "    all_texts.extend(texts) \n",
    "    print(f'Document {i+1} has {len(texts)} chunks') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(all_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6e. `MarkdownTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\`'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\`'\n",
      "/tmp/ipykernel_53237/4258703455.py:1: SyntaxWarning: invalid escape sequence '\\`'\n",
      "  markdown_text = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "markdown_text = \"\"\"\n",
    "#\n",
    "\n",
    "# Welcome to My Blog!\n",
    "\n",
    "## Introduction\n",
    "Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python, Java, and JavaScript.\n",
    "\n",
    "Here's a list of my favorite programming languages:\n",
    "\n",
    "1. Python\n",
    "2. JavaScript\n",
    "3. Java\n",
    "\n",
    "You can check out some of my projects on [GitHub](https://github.com).\n",
    "\n",
    "## About this Blog\n",
    "In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on the latest technology trends, and occasional book reviews.\n",
    "\n",
    "Here's a small piece of Python code to say hello:\n",
    "\n",
    "\\``` python\n",
    "def say_hello(name):\n",
    "    print(f\"Hello, {name}!\")\n",
    "\n",
    "say_hello(\"John\")\n",
    "\\```\n",
    "\n",
    "Stay tuned for more updates!\n",
    "\n",
    "## Contact Me\n",
    "Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at johndoe@email.com.\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary:\n",
    "\n",
    "`split_text()`: Returns plain text chunks without any additional information.\n",
    "\n",
    "`create_documents()`: Returns a list of documents that may contain both text chunks and associated metadata, making it more useful for structured use cases like document processing, search, or indexing.\n",
    "\n",
    "\n",
    "> If you need just the text split into chunks, use split_text(). If you need structured documents with metadata (e.g., for further NLP processing or indexing), use create_documents()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='#\\n\\n# Welcome to My Blog!', metadata={}), Document(page_content='## Introduction', metadata={}), Document(page_content='Hello everyone! My name is **John Doe** and I am a _software developer_. I specialize in Python,', metadata={}), Document(page_content='Java, and JavaScript.', metadata={}), Document(page_content=\"Here's a list of my favorite programming languages:\\n\\n1. Python\\n2. JavaScript\\n3. Java\", metadata={}), Document(page_content='You can check out some of my projects on [GitHub](https://github.com).', metadata={}), Document(page_content='## About this Blog', metadata={}), Document(page_content=\"In this blog, I will share my journey as a software developer. I'll post tutorials, my thoughts on\", metadata={}), Document(page_content='the latest technology trends, and occasional book reviews.', metadata={}), Document(page_content=\"Here's a small piece of Python code to say hello:\", metadata={}), Document(page_content='\\\\``` python\\ndef say_hello(name):\\n    print(f\"Hello, {name}!\")\\n\\nsay_hello(\"John\")\\n\\\\', metadata={}), Document(page_content='```\\n\\nStay tuned for more updates!', metadata={}), Document(page_content='## Contact Me', metadata={}), Document(page_content='Feel free to reach out to me on [Twitter](https://twitter.com) or send me an email at', metadata={}), Document(page_content='johndoe@email.com.', metadata={})]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# CORRECT: With [] (list of documents): The method processes the list as one complete document, resulting in fewer splits.\n",
    "from langchain.text_splitter import MarkdownTextSplitter \n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs = markdown_splitter.create_documents([markdown_text])\n",
    "print(docs)\n",
    "print(len(docs))\n",
    "\n",
    "# INCORRECT: Without [] (single string): The method treats the entire string as a large block of text and splits it into a maximum number of chunks by single characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6f. `TokenTextSplitter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"Introduction\\n\\nLangChain is a framework for developing applications powered by large language models (LLMs).\\n\\nLangChain simplifies every stage of the LLM application lifecycle:\\n\\nDevelopment: Build your applications using LangChain's open-source building blocks, components, and third-party integrations. Use LangGraph to build stateful agents with first-class streaming and human-in-the-loop support.\\n\\nProductionization: Use LangSmith to inspect, monitor and evaluate your chains, so that you can continuously optimize and deploy with confidence.\\n\\nDeployment: Turn your LangGraph applications into production-ready APIs and Assistants with LangGraph Cloud.\\n\\nConcretely, the framework consists of the following open-source libraries:\\n\\nlangchain-core: Base abstractions and LangChain Expression Language.\\n\\nlangchain-community: Third party integrations.\\n\\nPartner packages (e.g. langchain-openai, langchain-anthropic, etc.): Some integrations have been further split into their own lightweight packages that only depend on langchain-core.\\n\\nlangchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\\n\\nLangGraph: Build robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph. Integrates smoothly with LangChain, but can be used without it.\\n\\nLangServe: Deploy LangChain chains as REST APIs.\\n\\nLangSmith: A developer platform that lets you debug, test, evaluate, and monitor LLM applications.\\n\\nnote\\n\\nThese docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.\\n\\nTutorials\\u200b\\n\\nIf you're looking to build something specific or are more of a hands-on learner, check out our tutorials section. This is the best place to get started.\\n\\nThese are the best ones to get started with:\\n\\nBuild a Simple LLM Application\\n\\nBuild a Chatbot\\n\\nBuild an Agent\\n\\nIntroduction to LangGraph\\n\\nExplore the full list of LangChain tutorials here, and check out other LangGraph tutorials here. To learn more about LangGraph, check out our first LangChain Academy course, Introduction to LangGraph, available here.\\n\\nHow-to guides\\u200b\\n\\nHere you’ll find short answers to “How do I….?” types of questions. These how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference. However, these guides will help you quickly accomplish common tasks.\\n\\nCheck out LangGraph-specific how-tos here.\\n\\nConceptual guide\\u200b\\n\\nIntroductions to all the key parts of LangChain you’ll need to know! Here you'll find high level explanations of all LangChain concepts.\\n\\nFor a deeper dive into LangGraph concepts, check out this page.\\n\\nAPI reference\\u200b\\n\\nHead to the reference section for full documentation of all classes and methods in the LangChain Python packages.\\n\\nEcosystem\\u200b\\n\\n🦜🛠️ LangSmith\\u200b\\n\\nTrace and evaluate your language model applications and intelligent agents to help you move from prototype to production.\\n\\n🦜🕸️ LangGraph\\u200b\\n\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it.\\n\\nAdditional resources\\u200b\\n\\nVersions\\u200b\\n\\nSee what changed in v0.3, learn how to migrate legacy code, read up on our versioning policies, and more.\\n\\nSecurity\\u200b\\n\\nRead up on security best practices to make sure you're developing safely with LangChain.\\n\\nIntegrations\\u200b\\n\\nLangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.\\n\\nContributing\\u200b\\n\\nCheck out the developer's guide for guidelines on contributing and help getting your dev environment set up.\\n\\nEdit this page\\n\\nWas this page helpful?\\n\\nYou can also leave detailed feedback on GitHub.\" metadata={'source': 'https://python.langchain.com/docs/introduction/'}\n"
     ]
    }
   ],
   "source": [
    "# SeleniumURLLoader: Loads the text from the specified URL\n",
    "from langchain.document_loaders import SeleniumURLLoader \n",
    "\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/introduction/\"\n",
    "    ,\"https://python.langchain.com/docs/versions/v0_3/\"\n",
    "]\n",
    "\n",
    "loader = SeleniumURLLoader(urls = urls)\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 has 18 chunks\n",
      "Document 2 has 61 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter \n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
    "\n",
    "all_texts = []\n",
    "for i, doc in enumerate(data): \n",
    "    doc_content = doc.page_content \n",
    "    texts = text_splitter.split_text(doc_content)\n",
    "    all_texts.extend(texts) \n",
    "    print(f'Document {i+1} has {len(texts)} chunks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Embeddings - Similarity Search and Vector Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7a. OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai \n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from langchain.embeddings import OpenAIEmbeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the documents\n",
    "documents = [\n",
    "    \"The cat is on the mat.\",\n",
    "    \"There is a cat on the mat.\",\n",
    "    \"The dog is in the yard.\",\n",
    "    \"There is a dog in the yard.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializae the OpenAIEmbeddings instance \n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "# Generate embeddings for the documents \n",
    "document_embeddings = embeddings.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.97331318 0.96955366 0.82846392 0.82959776]]\n",
      "[0.97331318 0.96955366 0.82846392 0.82959776]\n"
     ]
    }
   ],
   "source": [
    "# get embeddings for the query \n",
    "query = \"A cat is sitting on a mat.\"\n",
    "query_embedding = embeddings.embed_query(query) \n",
    "\n",
    "# perform a similarity search between the query's embedding and the documents' embeddings\n",
    "\n",
    "## similarity scores between the query and each document \n",
    "similarity_scores = cosine_similarity([query_embedding], document_embeddings)[0]\n",
    "# the [0] is to get the first row of the cosine similarity matrix -> flatten the matrix\n",
    "# the matrix is of shape (1, len(documents)) \n",
    "print(cosine_similarity([query_embedding], document_embeddings)) \n",
    "print(similarity_scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar document to the query 'A cat is sitting on a mat.':\n",
      "The cat is on the mat.\n"
     ]
    }
   ],
   "source": [
    "# find the most similar document \n",
    "most_similar_index = np.argmax(similarity_scores)  # argmax returns the index of the maximum value in a given array or sequence\n",
    "most_similar_document = documents[most_similar_index] \n",
    "\n",
    "print(f\"Most similar document to the query: '{query}':\")\n",
    "print(most_similar_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7b. Open-source Embedding Models - HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We chose `sentence-transformers/all-mpnet-base-v2`, a pre-trained model for converting sentences into semantically meaningful vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.38.0 (from sentence_transformers)\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence_transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (2.4.0+cpu)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.12/site-packages (from sentence_transformers) (1.14.0)\n",
      "Collecting huggingface-hub>=0.19.3 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sentence_transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.19.3->sentence_transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (73.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.38.0->sentence_transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.12/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed huggingface-hub-0.25.1 safetensors-0.4.5 sentence_transformers-3.1.1 tokenizers-0.19.1 transformers-4.44.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/codespace/.python/current/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline \n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "model_kwargs = {'device':'cpu'}\n",
    "hf = HuggingFaceEmbeddings(model_name=model_name\n",
    "                           ,model_kwargs=model_kwargs)\n",
    "\n",
    "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
    "doc_embeddings = hf.embed_documents(documents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7c. Cohere Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cohere multilingual model maps text to semantic vector space and enhances text similarity comprehension in multilingual applications.\n",
    "\n",
    "> This model, distinct from their English language model, employ dot product computations for improved performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-cohere\n",
      "  Downloading langchain_cohere-0.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: cohere<6.0,>=5.5.6 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-cohere) (5.9.4)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-cohere) (0.3.5)\n",
      "Collecting langchain-experimental>=0.3.0 (from langchain-cohere)\n",
      "  Downloading langchain_experimental-0.3.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: pandas>=1.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-cohere) (2.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-cohere) (2.9.2)\n",
      "Collecting tabulate<0.10.0,>=0.9.0 (from langchain-cohere)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (1.35.25)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (1.9.7)\n",
      "Requirement already satisfied: httpx>=0.21.2 in /home/codespace/.local/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.4.0)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (0.20.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.32.0.20240914)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (0.1.126)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/codespace/.local/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-core<0.4,>=0.3.0->langchain-cohere) (8.5.0)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-experimental>=0.3.0->langchain-cohere) (0.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pandas>=1.4.3->langchain-cohere) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.4.3->langchain-cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas>=1.4.3->langchain-cohere) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.25 in /home/codespace/.python/current/lib/python3.12/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (1.35.25)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain-cohere) (0.10.2)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (3.10)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain-cohere) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/codespace/.local/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-cohere) (3.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.3.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.5.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/codespace/.python/current/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-cohere) (3.10.7)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain-cohere) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain-cohere) (2.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (0.25.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.12.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (2024.2.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain-cohere) (4.66.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (0.3.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.0.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/codespace/.python/current/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental>=0.3.0->langchain-cohere) (1.0.0)\n",
      "Downloading langchain_cohere-0.3.0-py3-none-any.whl (43 kB)\n",
      "Downloading langchain_experimental-0.3.0-py3-none-any.whl (206 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate, langchain-experimental, langchain-cohere\n",
      "Successfully installed langchain-cohere-0.3.0 langchain-experimental-0.3.0 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U langchain-cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "\n",
    "# Initialize the CohereEmbeddings object \n",
    "cohere = CohereEmbeddings(model = 'embed-multilingual-v2.0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello from Cohere!\n",
      "Embedding: [0.23461914, 0.50146484, -0.048828125, 0.13989258, -0.18029785]\n",
      "Text: مرحبًا من كوهير!\n",
      "Embedding: [0.25317383, 0.30004883, 0.0104904175, 0.12573242, -0.18273926]\n",
      "Text: Hallo von Cohere!\n",
      "Embedding: [0.10266113, 0.28320312, -0.050201416, 0.23706055, -0.07159424]\n",
      "Text: Bonjour de Cohere!\n",
      "Embedding: [0.15185547, 0.28173828, -0.057281494, 0.11743164, -0.04385376]\n",
      "Text: ¡Hola desde Cohere!\n",
      "Embedding: [0.25146484, 0.43139648, -0.0859375, 0.24682617, -0.11706543]\n",
      "Text: Olá do Cohere!\n",
      "Embedding: [0.18664551, 0.39038086, -0.045898438, 0.14562988, -0.11254883]\n",
      "Text: Ciao da Cohere!\n",
      "Embedding: [0.115722656, 0.43310547, -0.026168823, 0.14575195, 0.07080078]\n",
      "Text: 您好，来自 Cohere！\n",
      "Embedding: [0.24609375, 0.30859375, -0.111694336, 0.26635742, -0.051086426]\n",
      "Text: कोहेरे से नमस्ते!\n",
      "Embedding: [0.1932373, 0.6352539, 0.03213501, 0.117370605, -0.26098633]\n",
      "Text: Xin chào Cohere!\n",
      "Embedding: [0.29492188, 0.38793945, -0.013412476, 0.19311523, -0.0993042]\n"
     ]
    }
   ],
   "source": [
    "# Define a list of texts\n",
    "texts = [\n",
    "    \"Hello from Cohere!\",\n",
    "    \"مرحبًا من كوهير!\",\n",
    "    \"Hallo von Cohere!\",  \n",
    "    \"Bonjour de Cohere!\",\n",
    "    \"¡Hola desde Cohere!\",\n",
    "    \"Olá do Cohere!\",  \n",
    "    \"Ciao da Cohere!\",\n",
    "    \"您好，来自 Cohere！\",\n",
    "    \"कोहेरे से नमस्ते!\",\n",
    "    \"Xin chào Cohere!\"\n",
    "]\n",
    "\n",
    "# generate embeddings for the texts \n",
    "document_embeddings = cohere.embed_documents(texts)\n",
    "\n",
    "# print the embeddings \n",
    "for text, embedding in zip(texts, document_embeddings):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Embedding: {embedding[:5]}\") # print the first 5 elements of the embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7d. Deep Lake Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeplake\n",
      "  Downloading deeplake-3.9.23.tar.gz (617 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.0/617.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from deeplake) (1.26.4)\n",
      "Collecting pillow~=10.2.0 (from deeplake)\n",
      "  Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: boto3 in /home/codespace/.python/current/lib/python3.12/site-packages (from deeplake) (1.35.25)\n",
      "Collecting click (from deeplake)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting pathos (from deeplake)\n",
      "  Downloading pathos-0.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting humbug>=0.3.1 (from deeplake)\n",
      "  Downloading humbug-0.3.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from deeplake) (4.66.5)\n",
      "Collecting lz4 (from deeplake)\n",
      "  Downloading lz4-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting pyjwt (from deeplake)\n",
      "  Downloading PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pydantic in /home/codespace/.python/current/lib/python3.12/site-packages (from deeplake) (2.9.2)\n",
      "Collecting aioboto3>=10.4.0 (from deeplake)\n",
      "  Downloading aioboto3-13.1.1-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/codespace/.local/lib/python3.12/site-packages (from deeplake) (1.6.0)\n",
      "Collecting aiobotocore==2.13.1 (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
      "  Downloading aiobotocore-2.13.1-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting aiofiles>=23.2.1 (from aioboto3>=10.4.0->deeplake)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting botocore<1.34.132,>=1.34.70 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
      "  Downloading botocore-1.34.131-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (3.10.5)\n",
      "Collecting wrapt<2.0.0,>=1.10.10 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake)\n",
      "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting boto3 (from deeplake)\n",
      "  Downloading boto3-1.34.131-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from boto3->deeplake) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from boto3->deeplake) (0.10.2)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from humbug>=0.3.1->deeplake) (2.32.3)\n",
      "Collecting ppft>=1.7.6.8 (from pathos->deeplake)\n",
      "  Downloading ppft-1.7.6.8-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dill>=0.3.8 (from pathos->deeplake)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pox>=0.3.4 (from pathos->deeplake)\n",
      "  Downloading pox-0.3.4-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting multiprocess>=0.70.16 (from pathos->deeplake)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->deeplake) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->deeplake) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic->deeplake) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->humbug>=0.3.1->deeplake) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->humbug>=0.3.1->deeplake) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->humbug>=0.3.1->deeplake) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore==2.13.1->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake) (1.16.0)\n",
      "Downloading aioboto3-13.1.1-py3-none-any.whl (34 kB)\n",
      "Downloading aiobotocore-2.13.1-py3-none-any.whl (76 kB)\n",
      "Downloading boto3-1.34.131-py3-none-any.whl (139 kB)\n",
      "Downloading humbug-0.3.2-py3-none-any.whl (15 kB)\n",
      "Downloading pillow-10.2.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading lz4-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathos-0.3.2-py3-none-any.whl (82 kB)\n",
      "Downloading PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading botocore-1.34.131-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pox-0.3.4-py3-none-any.whl (29 kB)\n",
      "Downloading ppft-1.7.6.8-py3-none-any.whl (56 kB)\n",
      "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
      "Downloading wrapt-1.16.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (87 kB)\n",
      "Building wheels for collected packages: deeplake\n",
      "  Building wheel for deeplake (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deeplake: filename=deeplake-3.9.23-py3-none-any.whl size=740182 sha256=e1c9a37e4967e6cd74b0d08c7d36de1460502fec45530ae260a4323ccae9e74e\n",
      "  Stored in directory: /home/codespace/.cache/pip/wheels/4a/ec/cc/48c505bc12fb5e8982f933401f8f4755558fce86d7f9a34d32\n",
      "Successfully built deeplake\n",
      "Installing collected packages: wrapt, pyjwt, ppft, pox, pillow, lz4, dill, click, aioitertools, aiofiles, multiprocess, humbug, botocore, pathos, aiobotocore, boto3, aioboto3, deeplake\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: pillow 10.4.0\n",
      "    Uninstalling pillow-10.4.0:\n",
      "      Successfully uninstalled pillow-10.4.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.25\n",
      "    Uninstalling botocore-1.35.25:\n",
      "      Successfully uninstalled botocore-1.35.25\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.25\n",
      "    Uninstalling boto3-1.35.25:\n",
      "      Successfully uninstalled boto3-1.35.25\n",
      "Successfully installed aioboto3-13.1.1 aiobotocore-2.13.1 aiofiles-24.1.0 aioitertools-0.12.0 boto3-1.34.131 botocore-1.34.131 click-8.1.7 deeplake-3.9.23 dill-0.3.8 humbug-0.3.2 lz4-4.3.3 multiprocess-0.70.16 pathos-0.3.2 pillow-10.2.0 pox-0.3.4 ppft-1.7.6.8 pyjwt-2.9.0 wrapt-1.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install deeplake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings \n",
    "from langchain.vectorstores import DeepLake\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter \n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.chains import RetrievalQA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. split documents into chunks\n",
    "\n",
    "texts = [\n",
    "    \"Napoleon Bonaparte was born in 15 August 1769\",\n",
    "    \"Louis XIV was born in 5 September 1638\",\n",
    "    \"Lady Gaga was born in 28 March 1986\",\n",
    "    \"Michael Jeffrey Jordan was born in 17 February 1963\"\n",
    "] \n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0) \n",
    "docs = text_splitter.create_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedding function is deprecated and will be removed in the future. Please use embedding instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://bichpham102/langchain_course_embeddings already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 4 embeddings in 1 batches of size 4:: 100%|██████████| 1/1 [00:26<00:00, 26.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bichpham102/langchain_course_embeddings', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (4, 1536)  float32   None   \n",
      "    id        text      (4, 1)      str     None   \n",
      " metadata     json      (4, 1)      str     None   \n",
      "   text       text      (4, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ed38329e-7a89-11ef-a33a-6045bd2238a6',\n",
       " 'ed38342e-7a89-11ef-a33a-6045bd2238a6',\n",
       " 'ed383500-7a89-11ef-a33a-6045bd2238a6',\n",
       " 'ed3835b4-7a89-11ef-a33a-6045bd2238a6']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Get and Store embeddings in DeepLake\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# create a DeepLake dataset \n",
    "my_activeloop_org_id = \"bichpham102\" # TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_dataset_name = 'langchain_course_embeddings'\n",
    "dataset_path = f'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "\n",
    "# add documents to the DeepLake dataset \n",
    "db.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2719/4072583758.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  model = ChatOpenAI(model_name='gpt-3.5-turbo')\n"
     ]
    }
   ],
   "source": [
    "# 3. Create a Retriever, and incorporate it into a RetrievalQA chain\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "# instantiate the llm wrapper \n",
    "model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "# create the chain \n",
    "qa_chain = RetrievalQA.from_llm(model, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2719/4293071465.py:2: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  qa_chain.run('When was Lady Gaga born?')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Lady Gaga was born on 28 March 1986.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. Query the chain \n",
    "qa_chain.run('When was Lady Gaga born?') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Langchain Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8a. LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> create a bot to suggest contextually appropriate replacement words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMChain - 1 input per prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, LLMChain \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "prompt_template=PromptTemplate.from_template(\"What is a word to replace the following: {word}?\")\n",
    "# prompt_template=PromptTemplate.from_template(\"Correct the spelling of this Vietnamese word/phrase: {word}?\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Use the new RunnableSequence approach\n",
    "llm_chain = prompt_template | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='scarce' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 2, 'prompt_tokens': 18, 'total_tokens': 20, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-0807cedc-2ee0-4d8f-8831-d24072a2691f-0'\n",
      "scarce\n"
     ]
    }
   ],
   "source": [
    "# Use `invoke` for single input \n",
    "word = 'rare'\n",
    "response = llm_chain.invoke(word)\n",
    "print(response)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar\n",
      "Cleverness\n",
      "android\n"
     ]
    }
   ],
   "source": [
    "# Use `batch` for multiple inputs\n",
    "input_list = [\n",
    "    {\"word\": \"analogous\"},\n",
    "    {\"word\": \"intelligence\"},\n",
    "    {\"word\": \"robot\"}\n",
    "]\n",
    "\n",
    "response = llm_chain.batch(input_list)\n",
    "for r in response:\n",
    "    print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMChain - 2 input per prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air conditioner\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "template = \"\"\"Looking at the context of '{context}'. \\\n",
    "What is an appropriate word to replace the following: {word}?\"\"\" \n",
    "prompt_template=PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "llm_chain = prompt_template | llm \n",
    "\n",
    "# Example input data\n",
    "input_data = {\n",
    "    \"context\": \"object\",\n",
    "    \"word\": \"fan\"\n",
    "}\n",
    "\n",
    "# Invoke the chain for the input data\n",
    "response = llm_chain.invoke(input_data)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another Option: We can directly pass a prompt as a string to a Chain and initialize it using the `.from_string()` function as follows: `LLMChain.from_string(llm=llm, template=template)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quickly\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define your template directly as a string\n",
    "template = \"\"\"Looking at the context of '{context}', what is an appropriate word to replace the following: {word}?\"\"\"\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Initialize the chain using .from_string()\n",
    "llm_chain = LLMChain.from_string(llm=llm, template=template)\n",
    "\n",
    "# Example input data\n",
    "input_data = {\n",
    "    \"context\": \"The car was driving fast on the highway.\",\n",
    "    \"word\": \"fast\"\n",
    "}\n",
    "\n",
    "# Run the chain with the input data\n",
    "response = llm_chain.run(input_data)\n",
    "\n",
    "# Print the response\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8b. Conversational Chain (Memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8c. Sequential Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8d. Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8e. Custom Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

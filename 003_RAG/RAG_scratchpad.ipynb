{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Source: 'Building LLMs for Production' by Louis-Francois Bouchard, Louie Peters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Install related packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a LangChain Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader \n",
    "\n",
    "text =\"\"\" Google opens up its AI language model PaLM to challenge OpenAI and GPT-3 Google offers developers access to one of its most advanced AI language models: PaLM. The search giant is launching an API for PaLM alongside a number of AI enterprise tools it says will help businesses \"generate text, images, code, videos, audio, and more from simple natural language prompts.\"\n",
    "\n",
    "PaLM is a large language model, or LLM, similar to the GPT series created by OpenAI or Meta's LLaMA family of models. Google first announced PaLM in April 2022. Like other LLMs, PaLM is a flexible system that can potentially carry out all sorts of text generation and editing tasks. You could train PaLM to be a conversational chatbot like ChatGPT, for example, or you could use it for tasks like summarizing text or even writing code. (It's similar to features Google also announced today for its Workspace apps like Google Docs and Gmail.)\n",
    "\"\"\"\n",
    "\n",
    "# Write text to local file \n",
    "with open('my_file.txt','w') as file: \n",
    "    file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Use TextLoader to load text from local file \n",
    "\n",
    "loader = TextLoader('my_file.txt')      # Create an instance of TextLoader class \n",
    "docs_from_file = loader.load()          # Call the load method to load the text from the file\n",
    "\n",
    "print(len(docs_from_file))              # Print the number of documents loaded from the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Split the documents into Chunks with `CharacterTextSplitter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Chunk_overlap` is the number of characters that overlap between two chunks. \n",
    "\n",
    ">It preserves context and improves coherence by ensuring that important information is not cut off at the boundaries of chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 369, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter \n",
    "\n",
    "# Create a text splitter instance \n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "# Split the text into chunks\n",
    "docs = text_splitter.split_documents(docs_from_file)\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Setup a vector store & Create an embedding for each chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A vector store is a system to store embeddings, allowing us to query them.\n",
    "\n",
    "> Chunking is done because LLMs typically have a limited context window, and storing smaller chunks improves retrieval accuracy and ensures relevant sections are retrieved.\n",
    "\n",
    "We'll utilize the Deep Lake vector store, offered by Activeloop. They provide a cloud-based vector store solution, but other options like Chroma DB would also be suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the \"OPENAI_API_KEY\" environment variable.\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Lake Dataset in hub://bichpham102/langchain_course_indexers_retrievers already exists, loading from the storage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating 2 embeddings in 1 batches of size 2:: 100%|██████████| 1/1 [00:26<00:00, 26.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://bichpham102/langchain_course_indexers_retrievers', tensors=['embedding', 'id', 'metadata', 'text'])\n",
      "\n",
      "  tensor      htype      shape     dtype  compression\n",
      "  -------    -------    -------   -------  ------- \n",
      " embedding  embedding  (6, 1536)  float32   None   \n",
      "    id        text      (6, 1)      str     None   \n",
      " metadata     json      (6, 1)      str     None   \n",
      "   text       text      (6, 1)      str     None   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['252d5ad8-7839-11ef-93ec-000d3ac95fb9',\n",
       " '252d5c22-7839-11ef-93ec-000d3ac95fb9']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import DeepLake \n",
    "\n",
    "# Signup and Get your DeepLake API key  (https://app.activeloop.ai/), then save API Key / Secret before executing the following code \n",
    "\n",
    "# create a DeepLake dataset \n",
    "\n",
    "my_activeloop_org_id = \"bichpham102\" # TODO: use your organization id here. (by default, org id is your username)\n",
    "my_activeloop_dataset_name = 'langchain_course_indexers_retrievers'\n",
    "\n",
    "dataset_path = f'hub://{my_activeloop_org_id}/{my_activeloop_dataset_name}'\n",
    "db = DeepLake(dataset_path=dataset_path, embedding_function=embeddings)\n",
    "db.add_documents(docs)\n",
    "# the documents (docs) and their corresponding embeddings will be generated and stored in the new DeepLake dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create & work with a LangChain retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever() \n",
    "# calling the method as_retriever() on the vector store instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Once created the retriever, we can use the `RetrievalQA` class to define a question answering chain using external data source and start with question-answering. \n",
    "\n",
    "### 1. **`chain_type='stuff'`**\n",
    "   - **How it works**: This method takes **all retrieved documents** and \"stuffs\" them into a single prompt to the language model (LLM). It concatenates the documents and the query into one large input and then sends that to the LLM to generate a response.\n",
    "   - **Use case**: Suitable when the **total size of documents is small** enough to fit within the LLM’s context window (the maximum input size the model can process).\n",
    "   - **Advantages**: It's **simple and fast** because it only makes one call to the LLM.\n",
    "   - **Limitations**: This approach **fails** when the combined document size exceeds the context length of the model, leading to truncation or incomplete processing of the documents.\n",
    "   - **Best for**: Short documents or queries that need to be processed all at once【42†source】【45†source】.\n",
    "\n",
    "### 2. **`chain_type='map-reduce'`**\n",
    "   - **How it works**: Once the relevant chunks are retrieved from the vector store, the map-reduce process treats each retrieved chunk **independently**. It runs the LLM on each chunk to generate partial answers (the map step), and then **aggregates** (reduces) these partial results to form a final answer (the reduce step).\n",
    "   - **Use case**: Ideal for scenarios where the documents are **too large** to fit into the context window at once.\n",
    "   - **Advantages**: It allows for **scaling** to larger datasets because each chunk can be processed separately. It reduces the risk of important information being missed due to context limitations.\n",
    "   - **Limitations**: This approach may lead to **inconsistencies** across chunks or loss of context between parts of the document.\n",
    "   - **Best for**: Handling larger documents that exceed the model’s context window【43†source】【44†source】.\n",
    "\n",
    "### 3. **`chain_type='refine'`**\n",
    "   - **How it works**: In this method, the LLM processes the first document, generates an answer, and then **iteratively refines** that answer by incorporating information from each subsequent document. Each document is used to improve or adjust the previous answer.\n",
    "   - **Use case**: Best for situations where you want the model to **build upon** or **refine** an answer by looking at documents one-by-one.\n",
    "   - **Advantages**: This method is beneficial when **context continuity** is essential because the model refines its response with each new document. It ensures that **new insights** from each document are incorporated into the final answer.\n",
    "   - **Limitations**: It can be **slow** as the model processes each document individually and performs multiple iterations.\n",
    "   - **Best for**: Complex queries where iterative improvements in the answer based on additional information are crucial【42†source】【45†source】.\n",
    "\n",
    "### Summary of Use Cases:\n",
    "- **`stuff`**: Use when documents are small enough to fit into the context window. Simple and fast but limited by input size.\n",
    "- **`map-reduce`**: Ideal for large documents that need to be broken into chunks. Efficient for handling large datasets but may lead to slight inconsistencies.\n",
    "- **`refine`**: Best for complex answers that require refinement over time by integrating information from multiple documents. Provides thoroughness but is slower due to multiple iterations.\n",
    "\n",
    "These methods allow you to tailor the retrieval and answering process based on the size and complexity of the documents you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo') \n",
    "\n",
    "# create a retrieval chain \n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm\n",
    "    ,chain_type='stuff'\n",
    "    ,retriever=retriever \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google plans to challenge OpenAI by offering developers access to its advanced AI language model, PaLM. By launching an API for PaLM and providing AI enterprise tools, Google aims to help businesses generate text, images, code, videos, audio, and more from simple natural language prompts. PaLM is designed to be a flexible system that can carry out various text generation and editing tasks, similar to the GPT series created by OpenAI.\n"
     ]
    }
   ],
   "source": [
    "query = 'How Google plans to challenge OpenAI?'\n",
    "response = qa_chain.run(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever \n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# create a GPT-3 wrapper instance \n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo') \n",
    "\n",
    "# create compressor for the retriever \n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor\n",
    "    ,base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google opens up its AI language model PaLM to challenge OpenAI\n"
     ]
    }
   ],
   "source": [
    "# retrieving compressed documents \n",
    "query = 'How Google plans to challenge OpenAI?'\n",
    "retrieved_docs = compression_retriever.get_relevant_documents(query)\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
